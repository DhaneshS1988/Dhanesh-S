# -*- coding: utf-8 -*-
"""Assessment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RPSj4eewnCwbsz9hIv6brVRthzKd5JUL
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

#preprocessing is to be carried out in test and train file seperately

test=pd.read_csv("test.csv")
train=pd.read_csv("train.csv")

test.head()

test.dtypes

#there are both int and object data here

test.describe()

test.isna().sum()

#null values are found in education and previous year rating

test.info()

test.education.value_counts()

test['education'] = test['education'].fillna(test['education'].mode()[0])

test.isna().sum()

plt.hist(train['previous_year_rating'])
plt.title("previous year rating")
plt.xlabel("year")
plt.ylabel('rating')
plt.show()

test['previous_year_rating']=train['previous_year_rating'].fillna(test['previous_year_rating'].mode()[0])



#null values has been filled

#doing encoding now

#label encoding

test.dtypes

from sklearn import preprocessing
label_encoder = preprocessing.LabelEncoder()

test['department']= label_encoder.fit_transform(test['department'])

test.head()

test['region']= label_encoder.fit_transform(test['region'])

test['education']= label_encoder.fit_transform(test['education'])

test['gender']= label_encoder.fit_transform(test['gender'])

test['recruitment_channel']= label_encoder.fit_transform(test['recruitment_channel'])

test.head()

#DROPPING EMPLOYEE ID
test.drop(['employee_id'],axis=1)

#now doign min-max scaling

from sklearn.preprocessing import MinMaxScaler
min_max=MinMaxScaler()

test.dtypes

test.loc[:,['age','previous_year_rating','length_of_service','avg_training_score']]=min_max.fit_transform(test[['age','previous_year_rating','length_of_service','avg_training_score']])

test.head()

test.isna().sum()

#now performign the preprocessign steps for train dataset given

train.head()

train.isna().sum()

train['education'] = train['education'].fillna(train['education'].mode()[0])

train.isna().sum()

#null values have been filled

#doing encoding now

train['department']= label_encoder.fit_transform(train['department'])

train['region']= label_encoder.fit_transform(train['region'])
train['education']= label_encoder.fit_transform(train['education'])
train['gender']= label_encoder.fit_transform(train['gender'])

train.head()

train['recruitment_channel']= label_encoder.fit_transform(train['recruitment_channel'])

#now doing min-max scaling

train.loc[:,['age','previous_year_rating','length_of_service','avg_training_score']]=min_max.fit_transform(train[['age','previous_year_rating','length_of_service','avg_training_score']])

train.head()

#dropping employee id
train.drop(['employee_id'],axis=1)

#now modelling various models

combine = pd.concat([train,test], sort = False , ignore_index= True)

combine.head()

combine.drop(['employee_id'],axis=1)

combine.isna().sum()

combine.previous_year_rating.fillna(combine.previous_year_rating.median(),inplace=True)

combine.is_promoted.fillna(combine.is_promoted.median(),inplace=True)

combine.recuitment_channel.fillna(combine.recuitment_channel.median(),inplace=True)

combine.isna().sum()

combine.head()

from sklearn.model_selection import train_test_split,cross_val_score

from sklearn.linear_model import LogisticRegression

x=combine.drop(columns=['is_promoted'],axis=1)
y=combine['is_promoted']

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=42)

log_reg=LogisticRegression()
log_reg.fit(x_train,y_train)

y_pred=log_reg.predict(x_test)

from sklearn.metrics import f1_score,confusion_matrix,accuracy_score

confusion_matrix(y_test,y_pred)

print('Accuracy_score:',accuracy_score(y_test,y_pred))

f1_score(y_test, y_pred, zero_division=1)

#decision tree classifier

from sklearn.tree import DecisionTreeClassifier

decision_tree=DecisionTreeClassifier(criterion='gini')
decision_tree.fit(x_train,y_train)
y_pred=decision_tree.predict(x_test)
accuracy=accuracy_score(y_pred,y_test)
print("accuracy score:",accuracy*100)

f1_score(y_test, y_pred, zero_division=1)

from sklearn.ensemble import RandomForestClassifier

random_for=RandomForestClassifier()
random_for.fit(x_train,y_train)
y_pred=random_for.predict(x_test)
accuracy=accuracy_score(y_pred,y_test)
print('Accuracy score is:',accuracy*100)

f1_score(y_test, y_pred, zero_division=1)

from sklearn.neighbors import KNeighborsClassifier

metric_k=[]
neighbors=np.arange(3,15)

for k in neighbors:
  classifier=KNeighborsClassifier(n_neighbors=k,metric='minkowski',p=2)
  classifier.fit(x_train,y_train)
  y_pred=classifier.predict(x_test)
  acc=accuracy_score(y_test,y_pred)
  metric_k.append(acc)

plt.plot(neighbors,metric_k,'o-')
plt.xlabel('k values')
plt.ylabel('accuracy')
plt.grid()

classifier=KNeighborsClassifier(n_neighbors=10,metric='minkowski',p=2)
classifier.fit(x_train,y_train)

y_pred_knn=classifier.predict(x_test)

accuracy_score(y_pred_knn,y_test)

from google.colab import files
combine.to_csv('output.csv', encoding = 'utf-8-sig')
files.download('output.csv')

